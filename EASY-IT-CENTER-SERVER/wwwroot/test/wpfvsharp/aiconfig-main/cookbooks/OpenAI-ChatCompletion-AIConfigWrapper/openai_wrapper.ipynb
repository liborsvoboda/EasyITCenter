{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easily create an AIConfig from existing Openai code\n",
    "1. Basic usage\n",
    "2. Load existing aiconfig and continue\n",
    "3. Capture function calling\n",
    "4. Use Client API\n",
    "5. Save to existing AIConfig (no JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AIConfig package\n",
    "!pip3 install python-aiconfig\n",
    "\n",
    "# Create ~/.env file with this line: export OPENAI_API_KEY=<your key here>\n",
    "# You can get your key from https://platform.openai.com/api-keys \n",
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai\n",
    "\n",
    "output_path = \"my-first-aiconfig.json\"\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=output_path,\n",
    ")\n",
    "\n",
    "def run_my_existing_openai_app(user_message: str):\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 1,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": user_message,\n",
    "                \"role\": \"user\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) # Creates a config saved to default path `aiconfig.json`\n",
    "    print(\"Chat Completion Response: \")\n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_openai_app(\"Tell me a joke about apples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"my-first-aiconfig.json now represents your existing app:\\n\")\n",
    "result = json.load(open(output_path))\n",
    "print(\n",
    "    json.dumps(result, indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"Zoom in on the prompt:\\n\")\n",
    "print(\n",
    "    json.dumps(result[\"prompts\"][0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continue from existing aiconfig file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_openai_app(\"Tell a joke about apples in Shakespearian English.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"my-first-aiconfig.json has your existing prompts:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"And your new prompt:\\n\")\n",
    "print(\n",
    "    json.dumps(prompts[1], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Capture function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My existing app using function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Call Capture\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "output_path = \"my-function-calling-aiconfig.json\"\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=output_path,\n",
    ")\n",
    "\n",
    "\n",
    "def get_current_weather(location: str, unit: str) -> dict[str, Any]:\n",
    "    return { \"temperature\": 22, \"unit\": \"celsius\", \"description\": \"Sunny\" }\n",
    "\n",
    "\n",
    "def run_my_existing_weather_function_calling_app():\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo-0613\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"}],\n",
    "        \"functions\": [\n",
    "            {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) \n",
    "\n",
    "    function_call_response = get_current_weather(location=\"Boston\", unit=\"celsius\")\n",
    "    print(response)\n",
    "\n",
    "    completion_params = {\n",
    "      \"model\": \"gpt-3.5-turbo-0613\",\n",
    "      \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"},\n",
    "        {\"role\": \"assistant\", \"content\": 'null', \"function_call\": {\n",
    "              \"name\": \"get_current_weather\",\n",
    "              \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
    "            }},\n",
    "        {\"role\": \"function\", \"name\": \"get_current_weather\", \"content\": str(function_call_response)}\n",
    "\n",
    "      ],\n",
    "      \"functions\": [\n",
    "        {\n",
    "          \"name\": \"get_current_weather\",\n",
    "          \"description\": \"Get the current weather in a given location\",\n",
    "          \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "              },\n",
    "              \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) \n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_weather_function_calling_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inspect my-function-calling-aiconfig.json:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Client API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai_client\n",
    "\n",
    "\n",
    "output_path = \"my-aiconfig-from-Client-API.json\"\n",
    "\n",
    "client = get_completion_create_wrapped_openai_client(output_path)\n",
    "\n",
    "def run_my_existing_client_api_app():\n",
    "    completion_params = {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 1,\n",
    "                \"stream\": False,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"content\": \"Compare and contrast bananas and cucumbers.\",\n",
    "                        \"role\": \"user\",\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "    response = client.chat.completions.create(**completion_params)\n",
    "    print(type(response))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your code as usual\n",
    "run_my_existing_client_api_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(f\"Inspect {output_path}:\\n\")\n",
    "\n",
    "prompts = json.load(open(output_path))[\"prompts\"]\n",
    "print(\n",
    "    json.dumps(prompts[0], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save to existing AIConfig (no JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig.ChatCompletion import get_completion_create_wrapped_openai\n",
    "from aiconfig.Config import AIConfigRuntime\n",
    "\n",
    "existing_aiconfig = AIConfigRuntime.create()\n",
    "\n",
    "# replace openai import with this\n",
    "openai = get_completion_create_wrapped_openai(\n",
    "    output_aiconfig_ref=existing_aiconfig,\n",
    ")\n",
    "\n",
    "def run_my_existing_openai_app(user_message: str):\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 1,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": user_message,\n",
    "                \"role\": \"user\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = openai.chat.completions.create(**completion_params) # Creates a config saved to default path `aiconfig.json`\n",
    "    print(\"Chat Completion Response: \")\n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_my_existing_openai_app(\"Are tomatoes fruits?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await existing_aiconfig.run(\"prompt_0\")\n",
    "\n",
    "print(\"Result:\")\n",
    "print(existing_aiconfig.get_output_text(\"prompt_0\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
