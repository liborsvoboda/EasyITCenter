{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-aiconfig in /Users/saqadri/lm/aiconfig/python/src (1.0.0)\n",
      "Collecting requests==2.30.0 (from python-aiconfig)\n",
      "  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests==2.30.0->python-aiconfig) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests==2.30.0->python-aiconfig) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests==2.30.0->python-aiconfig) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests==2.30.0->python-aiconfig) (2023.7.22)\n",
      "Installing collected packages: requests\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed requests-2.30.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#1 install the package\n",
    "%pip install python-aiconfig\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample AI Config Representation\n",
    "```json\n",
    "{\n",
    "    \"name\": \"gpt4 as your data engineer\",\n",
    "    \"description\": \"\",\n",
    "    \"schema_version\": \"latest\",\n",
    "    \"metadata\": {\n",
    "        \"parameters\": {},\n",
    "        \"models\": {\n",
    "            \"gpt-3.5-turbo\": {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 1\n",
    "            },\n",
    "            \"gpt-4\": {\n",
    "                \"model\": \"gpt-4\",\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 1,\n",
    "                \"system_prompt\": \"You are an expert at SQL. You will output nicely formatted SQL code with labels on columns. You will provide a short 1-2 sentence summary on the code. Name columns as one word using underscore and lowercase. Format Output in markdown ### SQL Query code block with SQL Query &nbsp; ### Summary short summary on code\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"prompts\": [\n",
    "        {\n",
    "            \"name\": \"prompt1\",\n",
    "            \"input\": \"Write me a {{sql_language}} query to get this final output: {{output_data}}. Use the tables relationships defined here: {{table_relationships}}.\",\n",
    "            \"metadata\": {\n",
    "                \"model\": {\n",
    "                    \"name\": \"gpt-3.5-turbo\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"prompt2\",\n",
    "            \"input\": \"Translate the following into PostgreSQL code:\\n {{prompt1.output}}\",\n",
    "            \"metadata\": {\n",
    "                \"model\": {\n",
    "                    \"name\": \"gpt-4\",\n",
    "                    \"settings\": {\n",
    "                        \"model\": \"gpt-4\",\n",
    "                        \"top_p\": 1,\n",
    "                        \"max_tokens\": 3000,\n",
    "                        \"temperature\": 1\n",
    "                        \n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load A Config\n",
    "\n",
    "from aiconfig import AIConfigRuntime\n",
    "\n",
    "config_file_path = \"parametrized_data_config.json\"\n",
    "config = AIConfigRuntime.load(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt1 takes in 3 parameters: sql_language, output_data, table_relationships\n",
    "# Lets define some parameter values for prompt1\n",
    "\n",
    "prompt1_params = {\n",
    "    \"sql_language\": \"mysql\",\n",
    "    \"output_data\": \"user_name, user_email, trial, num_trial_steps, num_trial_steps_params. output granularity is the trial_id.\",\n",
    "    \"table_relationships\": \"user table, trial table, trial_step table. a user can create many trials. each trial has many trial_steps. a trial_step has parameters if metadata[0] (json) has a non-null parameters value. \"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_tokens': 3000,\n",
      " 'messages': [{'content': 'Write me a mysql query to get this final output: '\n",
      "                          'user_name, user_email, trial, num_trial_steps, '\n",
      "                          'num_trial_steps_params. output granularity is the '\n",
      "                          'trial_id.. Use the tables relationships defined '\n",
      "                          'here: user table, trial table, trial_step table. a '\n",
      "                          'user can create many trials. each trial has many '\n",
      "                          'trial_steps. a trial_step has parameters if '\n",
      "                          'metadata[0] (json) has a non-null parameters value. '\n",
      "                          '.',\n",
      "               'role': 'user'}],\n",
      " 'model': 'gpt-3.5-turbo',\n",
      " 'temperature': 1,\n",
      " 'top_p': 1}\n"
     ]
    }
   ],
   "source": [
    "# What if I want to see the completion parameters of a prompt before inference? This is useful for debugging, calculating cost of api calls, etc.\n",
    "\n",
    "prompt1_completion_params = await config.resolve(\"prompt1\", prompt1_params) \n",
    "\n",
    "import pprint\n",
    "# Lets take a look at the completion parameters\n",
    "pprint.pprint(prompt1_completion_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExecuteResult(output_type='execute_result', execution_count=0.0, data=<OpenAIObject at 0x124d463f0> JSON: {\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"To achieve the desired output, the following MySQL query can be used:\\n\\n```sql\\nSELECT u.user_name, u.user_email, t.trial_id, COUNT(ts.trial_step_id) AS num_trial_steps, SUM(CASE WHEN JSON_EXTRACT(ts.metadata, '$[0].parameters') IS NOT NULL THEN 1 ELSE 0 END) AS num_trial_steps_params\\nFROM user u\\nINNER JOIN trial t ON u.user_id = t.user_id\\nLEFT JOIN trial_step ts ON t.trial_id = ts.trial_id\\nGROUP BY u.user_id, t.trial_id;\\n```\\n\\nExplanation:\\n1. The query starts by selecting the required columns: user_name, user_email, trial_id.\\n2. The COUNT function is used with the trial_step table to count the number of trial_steps associated with each trial_id.\\n3. The SUM function is used with a CASE statement to count the number of trial_steps that have non-null parameters in the metadata column (using JSON_EXTRACT).\\n4. The query uses INNER JOIN between the user and trial tables based on the user_id relationship.\\n5. It uses LEFT JOIN between the trial and trial_step tables based on the trial_id relationship.\\n6. Finally, the query groups the results by user_id and trial_id to get the desired granularity.\\n\\nNote: Replace the table and column names with the actual names in your database schema.\"\n",
      "}, metadata={'id': 'chatcmpl-89HA27xKw6iiA1Rvv0EG40GS3fohd', 'object': 'chat.completion', 'created': 1697221610, 'model': 'gpt-3.5-turbo-0613', 'usage': <OpenAIObject at 0x124d47350> JSON: {\n",
      "  \"prompt_tokens\": 94,\n",
      "  \"completion_tokens\": 280,\n",
      "  \"total_tokens\": 374\n",
      "}, 'finish_reason': 'stop'})]\n"
     ]
    }
   ],
   "source": [
    "# Great! Now lets run prompt1 with the above parameters. Make sure to have your api credentials set.\n",
    "import asyncio\n",
    "\n",
    "prompt1_output = await config.run(\"prompt1\", prompt1_params)\n",
    "print(prompt1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To achieve the desired output, use the JOIN keyword to join the user table, trial table, and trial_step table. Group by trial_id and calculate the number of trial_steps and trial_steps_params by using COUNT and CASE statements.\n",
      "\n",
      "Here's the MySQL query:\n",
      "\n",
      "SELECT u.user_name, u.user_email, t.trial_id as trial, COUNT(ts.trial_step_id) as num_trial_steps, \n",
      "       COUNT(CASE WHEN ts.metadata->\"$.parameters\" IS NOT NULL THEN 1 END) as num_trial_steps_params\n",
      "FROM user u\n",
      "JOIN trial t ON u.user_id = t.user_id\n",
      "JOIN trial_step ts ON t.trial_id = ts.trial_id\n",
      "GROUP BY t.trial_id, u.user_name, u.user_email;"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ExecuteResult(output_type='execute_result', execution_count=0.0, data={'role': 'assistant', 'content': 'To achieve the desired output, use the JOIN keyword to join the user table, trial table, and trial_step table. Group by trial_id and calculate the number of trial_steps and trial_steps_params by using COUNT and CASE statements.\\n\\nHere\\'s the MySQL query:\\n\\nSELECT u.user_name, u.user_email, t.trial_id as trial, COUNT(ts.trial_step_id) as num_trial_steps, \\n       COUNT(CASE WHEN ts.metadata->\"$.parameters\" IS NOT NULL THEN 1 END) as num_trial_steps_params\\nFROM user u\\nJOIN trial t ON u.user_id = t.user_id\\nJOIN trial_step ts ON t.trial_id = ts.trial_id\\nGROUP BY t.trial_id, u.user_name, u.user_email;'}, metadata={'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Streaming\n",
    "\n",
    "from aiconfig import InferenceOptions\n",
    "\n",
    "# Lets define a handler for streaming OpenAI API calls. This function will be called for every delta in the output stream.\n",
    "def print_stream_delta(data, accumulated_data, index: int):\n",
    "    \"\"\"\n",
    "    streamCallback function that prints the stream output to console.\n",
    "    \"\"\"\n",
    "    if \"content\" in data:\n",
    "        content = data['content']\n",
    "        print(content, end = \"\", flush=True)\n",
    "\n",
    "\n",
    "# Define inference options to stream the output of the API call.\n",
    "inference_options = InferenceOptions(stream_callback=print_stream_delta, stream=True)\n",
    "\n",
    "# Run with streaming enabled, as well as the other parameters we defined earlier.\n",
    "await config.run(\"prompt1\", prompt1_params, inference_options) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SQL Query\n",
      "\n",
      "```SQL\n",
      "SELECT\n",
      "    u.user_name as username,\n",
      "    u.user_email as useremail,\n",
      "    t.trial_id as trialid,\n",
      "    COUNT(ts.trial_step_id) AS num_trial_steps,\n",
      "    SUM(CASE WHEN (ts.metadata->'parameters') IS NOT NULL THEN 1 ELSE 0 END) AS num_trial_steps_params\n",
      "FROM\n",
      "    user u\n",
      "    INNER JOIN trial t ON u.user_id = t.user_id\n",
      "    LEFT JOIN trial_step ts ON t.trial_id = ts.trial_id\n",
      "GROUP BY\n",
      "    u.user_id,\n",
      "    t.trial_id;\n",
      "```\n",
      "&nbsp;\n",
      "### Summary \n",
      "\n",
      "The provided PostgreSQL query retrieves the username, user email and trial ID from the database. Also, it calculates the total number of trial steps and the count of trial steps that have parameters for each user and trial. The involvement of three tables in the query: user, trial and trial_step necessitates their join operations. The data is then grouped by user and trial ID."
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ExecuteResult(output_type='execute_result', execution_count=0.0, data={'role': 'assistant', 'content': \"### SQL Query\\n\\n```SQL\\nSELECT\\n    u.user_name as username,\\n    u.user_email as useremail,\\n    t.trial_id as trialid,\\n    COUNT(ts.trial_step_id) AS num_trial_steps,\\n    SUM(CASE WHEN (ts.metadata->'parameters') IS NOT NULL THEN 1 ELSE 0 END) AS num_trial_steps_params\\nFROM\\n    user u\\n    INNER JOIN trial t ON u.user_id = t.user_id\\n    LEFT JOIN trial_step ts ON t.trial_id = ts.trial_id\\nGROUP BY\\n    u.user_id,\\n    t.trial_id;\\n```\\n&nbsp;\\n### Summary \\n\\nThe provided PostgreSQL query retrieves the username, user email and trial ID from the database. Also, it calculates the total number of trial steps and the count of trial steps that have parameters for each user and trial. The involvement of three tables in the query: user, trial and trial_step necessitates their join operations. The data is then grouped by user and trial ID.\"}, metadata={'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chained Parameterized Prompts\n",
    "\n",
    "# prompt 2 uses prompt1's output. Prompt2 asks to translate into postgres\n",
    "await config.run(\"prompt2\", {}, inference_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': '',\n",
      " 'metadata': {'models': {'gpt-3.5-turbo': {'max_tokens': 3000,\n",
      "                                           'model': 'gpt-3.5-turbo',\n",
      "                                           'temperature': 1,\n",
      "                                           'top_p': 1},\n",
      "                         'gpt-4': {'max_tokens': 3000,\n",
      "                                   'model': 'gpt-4',\n",
      "                                   'system_prompt': 'You are an expert at SQL. '\n",
      "                                                    'You will output nicely '\n",
      "                                                    'formatted SQL code with '\n",
      "                                                    'labels on columns. You '\n",
      "                                                    'will provide a short 1-2 '\n",
      "                                                    'sentence summary on the '\n",
      "                                                    'code. Name columns as one '\n",
      "                                                    'word using underscore and '\n",
      "                                                    'lowercase. Format Output '\n",
      "                                                    'in markdown ### SQL Query '\n",
      "                                                    'code block with SQL Query '\n",
      "                                                    '&nbsp; ### Summary short '\n",
      "                                                    'summary on code',\n",
      "                                   'temperature': 1,\n",
      "                                   'top_p': 1}},\n",
      "              'parameters': {}},\n",
      " 'name': 'gpt4 as your data engineer',\n",
      " 'prompts': [{'input': 'Write me a {{sql_language}} query to get this final '\n",
      "                       'output: {{output_data}}. Use the tables relationships '\n",
      "                       'defined here: {{table_relationships}}.',\n",
      "              'metadata': {'model': {'name': 'gpt-3.5-turbo', 'settings': None},\n",
      "                           'parameters': {},\n",
      "                           'tags': None},\n",
      "              'name': 'prompt1'},\n",
      "             {'input': 'Translate the following into PostgreSQL code:\\n'\n",
      "                       ' {{prompt1.output}}',\n",
      "              'metadata': {'model': {'name': 'gpt-4',\n",
      "                                     'settings': {'max_tokens': 3000,\n",
      "                                                  'model': 'gpt-4',\n",
      "                                                  'temperature': 1,\n",
      "                                                  'top_p': 1}},\n",
      "                           'parameters': {},\n",
      "                           'tags': None},\n",
      "              'name': 'prompt2'},\n",
      "             {'input': 'How do transformers work?',\n",
      "              'metadata': {'model': 'gpt-3.5-turbo',\n",
      "                           'parameters': {},\n",
      "                           'remember_chat_context': False,\n",
      "                           'tags': None},\n",
      "              'name': 'prompt3'}],\n",
      " 'schema_version': 'latest'}\n"
     ]
    }
   ],
   "source": [
    "# CRUD API\n",
    "# Programatically add, delete, and update prompts in the config.\n",
    "import json\n",
    "from aiconfig.schema import Prompt\n",
    "\n",
    "# Lets define a new prompt\n",
    "# Chat History defaults to True. Lets explicitly set it to false.\n",
    "prompt = Prompt(**{\"name\": \"prompt3\", \n",
    "                   \"input\": \"How do transformers work?\", \n",
    "                   \"metadata\": {\n",
    "                       \"model\": \"gpt-3.5-turbo\",\n",
    "                       \"remember_chat_context\": False,\n",
    "                }})\n",
    "\n",
    "config.add_prompt(\"prompt3\", prompt)\n",
    "\n",
    "# Exports aiconfig to disk.\n",
    "config.save(\"updated_aiconfig.json\")\n",
    "\n",
    "# Let's check out the new update config\n",
    "\n",
    "with open('updated_aiconfig.json', 'r') as f:\n",
    "    parsed = json.load(f)\n",
    "    pprint.pprint(parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_tokens': 3000, 'top_p': 1, 'temperature': 1, 'model': 'gpt-3.5-turbo', 'messages': [{'content': 'How do transformers work?', 'role': 'user'}], 'stream': True}\n",
      "Transformers work on the principle of electromagnetic induction. They are made up of two coils, known as the primary and secondary coils, wound around a common iron core.\n",
      "\n",
      "When an alternating current (AC) passes through the primary coil, it generates a magnetic field around the iron core. This magnetic field constantly expands and contracts due to the alternating current.\n",
      "\n",
      "The changing magnetic field induces an alternating voltage in the secondary coil through electromagnetic induction. The voltage induced in the secondary coil depends on the turns ratio between the primary and secondary coils.\n",
      "\n",
      "The basic equation for understanding transformers is Vp/Vs = Np/Ns, where Vp and Vs represent the voltages on the primary and secondary coils respectively, and Np and Ns represent the number of turns in the primary and secondary coils respectively.\n",
      "\n",
      "Transformers are designed in such a way that the primary coil has fewer turns than the secondary coil in a step-up transformer, resulting in an increased voltage on the secondary side. Conversely, in a step-down transformer, the primary coil has more turns than the secondary coil, resulting in a reduced voltage on the secondary side.\n",
      "\n",
      "By changing the turns ratio, transformers can efficiently step up or step down voltages, allowing for transmission of electrical energy over long distances with minimal losses. They are widely used in power distribution networks, electrical appliances, and various electronic devices.\n"
     ]
    }
   ],
   "source": [
    "config2 = AIConfigRuntime.load(\"updated_aiconfig.json\")\n",
    "\n",
    "# Resolve\n",
    "print(await config2.resolve( \"prompt3\",{}, inference_options))\n",
    "\n",
    "# Run\n",
    "await config2.run(\"prompt3\", {})\n",
    "\n",
    "# Show output\n",
    "print(config2.get_output_text(\"prompt3\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
